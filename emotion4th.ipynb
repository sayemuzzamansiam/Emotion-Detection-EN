{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ahmI4R1fTZCv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/sayemuzzamansiam/Emotion-Detection-EN/main/dff.csv\""
      ],
      "metadata": {
        "id": "jIlmlcAmTlmY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(url)"
      ],
      "metadata": {
        "id": "aEwK6pa3URQb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the data\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyNmafVJUaWx",
        "outputId": "a9fda7d5-3e6f-4098-f715-44d06b5e27a2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       emotion  emotion_num                                       cleaned_text\n",
            "0      sadness           18                                          game hurt\n",
            "1   admiration            0  sexuality shouldnt grouping category makes dif...\n",
            "2         love           15                                    man love reddit\n",
            "3    gratitude           13  right considering important document know damn...\n",
            "4  disapproval            9  isnt big hes still quite popular ive heard thi...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the text data\n",
        "df['cleaned_text'] = df['cleaned_text'].astype(str)  # Ensure all entries are strings\n",
        "df['cleaned_text'].fillna('', inplace=True)  # Replace NaN values with empty strings\n",
        "\n",
        "X = df['cleaned_text'].values\n",
        "y = df['emotion_num'].values\n",
        "\n"
      ],
      "metadata": {
        "id": "XGHjZ_mYUbsR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(X)\n",
        "word_index = tokenizer.word_index\n",
        "X_sequences = tokenizer.texts_to_sequences(X)\n",
        "X_padded = pad_sequences(X_sequences, maxlen=100, padding='post', truncating='post')\n",
        "\n"
      ],
      "metadata": {
        "id": "retnAVilU6Cp"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the model\n"
      ],
      "metadata": {
        "id": "SmMdHXfcVnPR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=5000, output_dim=64, input_length=100),\n",
        "    Bidirectional(LSTM(64)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(len(np.unique(y)), activation='softmax')\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "teHHDy7GVr-4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "eU-El1VdV4yi"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBFMVIQ9V5-p",
        "outputId": "018e563d-1e55-4cc0-d9db-5e752721616d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 100, 64)           320000    \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 128)               66048     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 32)                4128      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 21)                693       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 390869 (1.49 MB)\n",
            "Trainable params: 390869 (1.49 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=3, validation_data=(X_test, y_test), batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyB-eMU-V-FR",
        "outputId": "203c46de-afde-4146-a21f-8257eebfcf85"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "4024/4024 [==============================] - 449s 110ms/step - loss: 2.3498 - accuracy: 0.3118 - val_loss: 2.1589 - val_accuracy: 0.3694\n",
            "Epoch 2/3\n",
            "4024/4024 [==============================] - 441s 110ms/step - loss: 2.0599 - accuracy: 0.3951 - val_loss: 2.0959 - val_accuracy: 0.3818\n",
            "Epoch 3/3\n",
            "4024/4024 [==============================] - 439s 109ms/step - loss: 1.9734 - accuracy: 0.4144 - val_loss: 2.0709 - val_accuracy: 0.3925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model.save('emotion_model.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIOGNS-ScYBP",
        "outputId": "c7776413-993a-47ea-94f7-b475713b0c37"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Loss: {loss}')\n",
        "print(f'Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fxd5_RR_bF8C",
        "outputId": "fde347be-7093-43d8-f991-8acfea9a8482"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1006/1006 [==============================] - 28s 27ms/step - loss: 2.0709 - accuracy: 0.3925\n",
            "Loss: 2.0709173679351807\n",
            "Accuracy: 0.39246878027915955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "model = tf.keras.models.load_model('emotion_model.h5')\n",
        "\n",
        "# Function to predict the emotion of a given sentence\n",
        "def predict_emotion(sentence):\n",
        "    # Preprocess the input sentence\n",
        "    sentence = sentence.lower()  # Convert to lowercase\n",
        "    sequence = tokenizer.texts_to_sequences([sentence])\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=100, padding='post', truncating='post')\n",
        "\n",
        "    # Predict the emotion\n",
        "    prediction = model.predict(padded_sequence)\n",
        "    predicted_class = np.argmax(prediction, axis=1)[0]\n",
        "\n",
        "    # Decode the predicted class into the corresponding emotion label\n",
        "    emotions = {index: label for index, label in enumerate(df['emotion'].unique())}\n",
        "    predicted_emotion = emotions[predicted_class]\n",
        "\n",
        "    return predicted_emotion\n",
        "\n",
        "# Example usage\n",
        "sentence = \"love you\"\n",
        "predicted_emotion = predict_emotion(sentence)\n",
        "print(f\"The predicted emotion for the sentence is: {predicted_emotion}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9DDlosqcDAR",
        "outputId": "5331e975-d071-4b25-9354-f78c026db204"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7d65375e39a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 784ms/step\n",
            "The predicted emotion for the sentence is: disgust\n"
          ]
        }
      ]
    }
  ]
}